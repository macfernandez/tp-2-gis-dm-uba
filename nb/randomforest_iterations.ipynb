{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2401d954-3687-439b-a668-c219a35a5d18",
   "metadata": {},
   "source": [
    "# Entrenamiento iterativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754fe51f-9bbc-4739-8e74-800473891684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import joblib\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pyproj import CRS\n",
    "from copy import deepcopy\n",
    "from sqlite3 import connect\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc50e70-5593-4c74-88ea-d72ddb3d3411",
   "metadata": {},
   "source": [
    "## Conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e2de2-f7c6-48cc-a57a-e3d53142d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiar según corresponda\n",
    "# train_sqlite_files debe contener los .sqlite generados a partir de la verdad de campo\n",
    "train_sqlite_files = glob('../data/verdad_campo_sqlite/*.sqlite')\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "\n",
    "for sf in train_sqlite_files:\n",
    "    file_name = os.path.basename(sf)\n",
    "    tile = re.search(r'\\d+',file_name).group()\n",
    "    cnx = connect(sf)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM output\", cnx)\n",
    "    df['tile_file'] = tile\n",
    "    train_data = pd.concat([train_data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832c5d5-7d01-49d2-bf7f-dadc2d34128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13016a21-cf58-4035-9ce7-fdb7ca399547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d309a3-f979-49bf-82a7-2e8b05d195fe",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea57b00-be8e-4751-b4b1-79890ca2c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_id2cultivo = dict((\n",
    "    train_data[['id','cultivo']]\n",
    "    .drop_duplicates()\n",
    "    .assign(id=lambda x: x.id.astype('int'))\n",
    "    .itertuples(index=False, name=None))\n",
    ")\n",
    "map_id2cultivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db2c36-a013-4133-8282-3a6392787db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_data.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee94fb21-48a6-4f1d-8157-ea254135a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_le2id = dict(zip(le.transform(le.classes_), list(map(int,le.classes_))))\n",
    "\n",
    "map_le2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08505a9b-5ec0-4763-a2a2-0e6198b8457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['id_le'] = le.transform(train_data.id)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5240870-6685-4670-a2aa-f304a7e154e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/randomforest_parameters.json','r') as f:\n",
    "    parameters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522359a-aceb-49a9-95cb-3499ae990d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_from_tile(in_raster):\n",
    "    with rasterio.open(in_raster) as src:\n",
    "        return(src.width, src.height, src.transform)\n",
    "\n",
    "def sliding_windows(size, step_size, width, height, whole=False):\n",
    "    \"\"\"Slide a window of +size+ by moving it +step_size+ pixels\"\"\"\n",
    "    w, h = size, size\n",
    "    sw, sh = step_size, step_size\n",
    "    end_i = height - h if whole else height\n",
    "    end_j = width - w if whole else width\n",
    "    for pos_i, i in enumerate(range(0, end_i, sh)):\n",
    "        for pos_j, j in enumerate(range(0, end_j, sw)):\n",
    "            real_w = w if whole else min(w, abs(width - j))\n",
    "            real_h = h if whole else min(h, abs(height - i))\n",
    "            yield Window(j, i, real_w, real_h), (pos_i, pos_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7ed7a-83fe-4a49-add2-7e22df78fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_train = pd.DataFrame()\n",
    "i = 0\n",
    "while True:\n",
    "    \n",
    "    # si el entrenamiento de la próxima iteración es mayor (en cantidad)\n",
    "    # a la data de entrenamiento, entonces toma el entrenamiento de la próxima\n",
    "    # iteración\n",
    "    # si no, toma la data de entrenamiento original (verdad de capo original)\n",
    "    # el entrenamiento de la próxima iteración es un df que se va enriqueciendo\n",
    "    # con la nueva verdad de campo predicha\n",
    "    # hago esto así porque, usando los tif, no sé cómo verificar si los pixeles\n",
    "    # están o no en la verdad de campo (entonces no quiero agregar la nueva\n",
    "    # verdad de camp al df que ya contiene la verdad de campo original\n",
    "    # porquee estaría duplicando los datos)\n",
    "    # así, primera iteración va a usar la data original y ya en la segunda\n",
    "    # iteración va a tomar el entrenamiento con la nueva verdad\n",
    "    if next_train.shape[0] >= train_data.shape[0]:\n",
    "        train_data = next_train\n",
    "\n",
    "    # arma carpeta para el output (i aumenta con las iteraciones)\n",
    "    n_iter = '{0:03d}'.format(i)\n",
    "    output_folder = os.path.join('..','model',f'randomforest_iterations_{n_iter}')\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "    os.mkdir(output_folder)\n",
    "    \n",
    "    # segmenta en train y test\n",
    "    X = train_data.filter(regex='band_').fillna(-99).to_numpy()\n",
    "    y = train_data['id_le'].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=.3, random_state=20220714, shuffle=True, stratify=y\n",
    "    )\n",
    "    \n",
    "    # instancia y entrena el modelo con set de entrenamiento\n",
    "    model = RandomForestClassifier(**parameters)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # predice sobre el conjunto de testeo\n",
    "    probas = model.predict_proba(X_test)\n",
    "    output_proba_file = os.path.join(output_folder, f'probas_{n_iter}.npy')\n",
    "    np.save(output_proba_file, probas)\n",
    "    \n",
    "    y_hat = probas.argmax(axis=1)\n",
    "    \n",
    "    # guarda métricas\n",
    "    cmatrix = confusion_matrix(y_test, y_hat, normalize='all')\n",
    "    output_cmpatrix_file = os.path.join(output_folder, f'cmatrix_{n_iter}.npy')\n",
    "    np.save(output_cmpatrix_file, cmatrix)\n",
    "    \n",
    "    report = classification_report(y_test, y_hat, output_dict=True)\n",
    "    output_report_file = os.path.join(output_folder, f'report_{n_iter}.json')\n",
    "    with open(output_report_file, 'w') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    kappa = cohen_kappa_score(y_test, y_hat)\n",
    "    output_kapp_file = os.path.join(output_folder, f'kappa_{n_iter}.txt')\n",
    "    with open(output_kapp_file, 'w') as f:\n",
    "        _ = f.write(str(kappa))\n",
    "    \n",
    "    # evalúa cantidad de aciertos y errores por nivel de confianza (cada 0.05)\n",
    "    predictions = pd.DataFrame({\n",
    "        'success':y_test==y_hat,\n",
    "        'score':probas.max(axis=1),\n",
    "    })\n",
    "    hits, confidence = np.histogram(predictions[predictions.success==True].score, 20, (0,1))\n",
    "    misses, _ = np.histogram(predictions[predictions.success==False].score, 20, (0,1))\n",
    "    hits_misses_df = (\n",
    "        pd\n",
    "        .DataFrame({'hit':hits, 'miss':misses, 'confidence':np.round(confidence[:-1], 2)})\n",
    "    )\n",
    "    output_success_file = os.path.join(output_folder, f'hits_misses_{n_iter}.csv')\n",
    "    hits_misses_df.to_csv(output_success_file, index=False)\n",
    "    \n",
    "    # selecciona el umbral\n",
    "    # umbral = score cuya cantidad de aciertos duplique la cantidad de errores + 1\n",
    "    threshold = hits_misses_df.loc[hits_misses_df.hit>(hits_misses_df.miss+1)*2,'confidence'].min()\n",
    "    output_threshold_file = os.path.join(output_folder, f'threshold_{n_iter}.txt')\n",
    "    with open(output_threshold_file, 'w') as f:\n",
    "        _ = f.write(str(threshold))\n",
    "\n",
    "    # instancia y entrena el modelo con set de entrenamiento + validación\n",
    "    iter_X = train_data.filter(regex='band_').fillna(-99999).to_numpy()\n",
    "    iter_y = train_data['id_le'].to_numpy()\n",
    "\n",
    "    model = RandomForestClassifier(**parameters)\n",
    "    model.fit(iter_X, iter_y)\n",
    "    output_model_file = os.path.join(output_folder, f'model_{n_iter}.joblib')\n",
    "    _ = joblib.dump(model, output_model_file)\n",
    "    \n",
    "    # levanta los .tif para predecir\n",
    "    pred_tif = glob('../data/feature_importance/*.tif')\n",
    "    \n",
    "    vc_len = X_train.shape[0] + X_test.shape[0]\n",
    "    new_vc_len = 0\n",
    "    for tif in pred_tif:\n",
    "        # detecta nombre del raster\n",
    "        # 12544.tif o 00000.tif\n",
    "        name_raster = os.path.basename(tif)\n",
    "        \n",
    "        print(f'+++ PREDICCIÓN PARA TILE: {name_raster}')\n",
    "        \n",
    "        # levanta la metadata del tif\n",
    "        width, height, transform = metadata_from_tile(tif)\n",
    "        # arma las ventanas de 100x100\n",
    "        windows = sliding_windows(100, 100, width, height)\n",
    "    \n",
    "        # si no está en la primera iteración\n",
    "        # busca el raster para enmascarar de la iteración anterior\n",
    "        if i>0:\n",
    "            prev_i = i-1\n",
    "            prev_iter = '{0:03d}'.format(prev_i)\n",
    "            prev_folder = output_folder.replace(n_iter, prev_iter)\n",
    "            prev_tif = os.path.join(prev_folder, name_raster)\n",
    "        \n",
    "        out_raster = os.path.join(output_folder, name_raster)\n",
    "        with rasterio.open(\n",
    "            out_raster, 'w', driver='GTiff', count=1,\n",
    "            width=width, height=height, dtype=np.float64, transform=transform,\n",
    "            crs=CRS.from_epsg(4326), compress='lzw') as dst:\n",
    "            \n",
    "            windows = list(windows)\n",
    "            windows_len = len(windows)\n",
    "            wn = 0\n",
    "            for window in windows:\n",
    "                print(f'... Prediciendo ventana {wn} de {windows_len}')\n",
    "                wn +=1\n",
    "                \n",
    "                win=window[0]\n",
    "                # abre ventana en el raster original\n",
    "                src = rasterio.open(tif)\n",
    "                img = src.read(window=win)\n",
    "                r,m,n = img.shape\n",
    "                \n",
    "                # arma un dataframe con la data para predecir\n",
    "                # shape: 10000 filas (100x100 pixeles) x 28 columnas (bandas)\n",
    "                tif_df = (\n",
    "                    pd.DataFrame(img.reshape(r,m*n))\n",
    "                    .T.fillna(-99)\n",
    "                )\n",
    "                tif_df.rename(columns={col:f'band_{col}' for col in tif_df.columns}, inplace=True)\n",
    "                \n",
    "                # si no está en la primera iteración\n",
    "                # arma un dataframe con la data para enmascarar\n",
    "                # shape: 10000 filas (100x100 pixeles) x 28 columnas (bandas)\n",
    "                if i>0:\n",
    "                    # abre ventana en el raster de la iteración previa\n",
    "                    # para enmascarar\n",
    "                    src_mask = rasterio.open(prev_tif)\n",
    "                    mask = src_mask.read(window=win) \n",
    "                    mask_df = pd.DataFrame(mask.reshape(r,m*n)).T\n",
    "                    tif2predict = deepcopy(tif_df[mask_df.id==-99])\n",
    "                # si está en la primera iteración\n",
    "                # toma el tif completo\n",
    "                else:\n",
    "                    tif2predict = deepcopy(tif_df)\n",
    "\n",
    "                #img_df.rename(columns={col:f'band_{col}' for col in img_df.columns}, inplace=True)\n",
    "                # agrega columna con predicciones\n",
    "                iter_prediction = model.predict_proba(tif2predict.to_numpy())\n",
    "                tif2predict['id_le'] = iter_prediction.argmax(axis=1)\n",
    "                tif2predict['score'] = iter_prediction.max(axis=1)\n",
    "\n",
    "                # si el score de la predicción supera al umbral\n",
    "                # agrega esa info al entrenamiento de la próxima iteración\n",
    "                new_vc = tif2predict[tif2predict.score>=threshold]\n",
    "                new_vc['id_le'] = new_vc.id_le.astype('int')\n",
    "                next_train = pd.concat([next_train, new_vc])\n",
    "                new_vc_len += new_vc.shape[0]\n",
    "                \n",
    "                # asigna id real\n",
    "                # enmascara las predicciones cuyo score no supera al umbral (-99)\n",
    "                # y lo guarda en un nuevo .tif\n",
    "                tif_df.loc[mask_df.id==-99, 'id'] = tif2predict.id_le.map(map_le2id)\n",
    "                tif_df.loc[mask_df.id==-99, 'score'] = tif2predict.score\n",
    "                tif_df.loc[tif_df.score<threshold, 'id'] = -99\n",
    "                tif_class = np.expand_dims(tif_df.id.to_numpy().reshape(n,m), axis=0)\n",
    "                dst.write(tif_class, window=win)\n",
    "                \n",
    "                # cuando se hace la predicción hay que revisar si hay un .tif del modelo para enmascarar\n",
    "                # (solo se predicen los pixeles que tengan NA)          \n",
    "    \n",
    "    output_vc_file = os.path.join(output_folder, f'verdad_campo_{n_iter}.txt')\n",
    "    with open(output_vc_file, 'w') as f:\n",
    "        _ = f.write(f'''verdad_campo_entrenamiento,{vc_len}\\nvedad_campo_nueva{new_vc_len}\\n''')\n",
    "    \n",
    "    \n",
    "    # imprime información\n",
    "    print(f'''\\n*** ITERACIÓN #{i:03d}\n",
    "    - Pixeles de entrenamiento: {X_train.shape[0]}\n",
    "    - Pixeles de validación: {X_test.shape[0]}\n",
    "    - Probabilidades sobre train guardadas en {output_proba_file}\n",
    "    - Umbral definido: {threshold}\n",
    "    - Pixeles del modelo final: {vc_len}\n",
    "    - Modelo guardado en: {output_model_file}\n",
    "    - Nueva verdad de campo predicha: {new_vc_len}''')\n",
    "    \n",
    "    if new_vc_len == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
