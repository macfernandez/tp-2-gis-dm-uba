{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2401d954-3687-439b-a668-c219a35a5d18",
   "metadata": {},
   "source": [
    "# Entrenamiento iterativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754fe51f-9bbc-4739-8e74-800473891684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import joblib\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pyproj import CRS\n",
    "from copy import deepcopy\n",
    "from sqlite3 import connect\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc50e70-5593-4c74-88ea-d72ddb3d3411",
   "metadata": {},
   "source": [
    "## Conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e2de2-f7c6-48cc-a57a-e3d53142d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiar según corresponda\n",
    "# train_sqlite_files debe contener los .sqlite generados a partir de la verdad de campo\n",
    "train_sqlite_files = glob('../data/selection_verdad_campo/*.sqlite')\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "\n",
    "for sf in train_sqlite_files:\n",
    "    file_name = os.path.basename(sf)\n",
    "    tile = re.search(r'\\d+',file_name).group()\n",
    "    cnx = connect(sf)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM output\", cnx)\n",
    "    df['tile_file'] = tile\n",
    "    train_data = pd.concat([train_data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832c5d5-7d01-49d2-bf7f-dadc2d34128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13016a21-cf58-4035-9ce7-fdb7ca399547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95c0c8-b127-40a2-bc35-199bb3f7183c",
   "metadata": {},
   "source": [
    "## Conjunto de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468b355-411e-4fad-bfbc-feaee30b74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiar según corresponda\n",
    "# pred_sqlite_files debe contener los .sqlite generados a partir de la máscara mask_agri_aoi\n",
    "pred_sqlite_files = glob('../data/selection_mask_agri_aoi/*.sqlite')\n",
    "\n",
    "pred_data = pd.DataFrame()\n",
    "\n",
    "for sf in pred_sqlite_files:\n",
    "    file_name = os.path.basename(sf)\n",
    "    tile = re.search(r'\\d+',file_name).group()\n",
    "    cnx = connect(sf)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM output\", cnx)\n",
    "    df['tile_file'] = tile\n",
    "    pred_data = pd.concat([pred_data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f460d-2a3d-4fc7-9a11-a362062b80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7f6ab-39b9-4c74-9205-107131a96fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6478d-8c03-457b-b12f-f98c3c5712a0",
   "metadata": {},
   "source": [
    "## Agregado de departamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9954e-54e0-4fe5-b38c-d8854742a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiar según corresponda\n",
    "# deptos_sqlite_files debe contener los .sqlite generados a partir de los departamentos\n",
    "deptos_sqlite_files = glob('../data/selection_departamentos_features/*.sqlite')\n",
    "\n",
    "deptos_data = pd.DataFrame()\n",
    "\n",
    "for sf in deptos_sqlite_files:\n",
    "    file_name = os.path.basename(sf)\n",
    "    tile = re.search(r'\\d+',file_name).group()\n",
    "    cnx = connect(sf)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM output\", cnx)\n",
    "    df['tile_file'] = tile\n",
    "    deptos_data = pd.concat([deptos_data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eafacb8-82ee-4394-916c-86c8a8da90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.merge(deptos_data[['ogc_fid','nombre','tile_file']], how='left', on=['ogc_fid','tile_file'])\n",
    "pred_data = pred_data.merge(deptos_data[['ogc_fid','nombre','tile_file']], how='left', on=['ogc_fid','tile_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf61bd2-bbdd-4722-886f-74e756bcf773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data está dentro de pred_data\n",
    "# saco esas filas de pred_data\n",
    "# así no predecimos con lo mismo con lo que entrenamos\n",
    "\n",
    "merged_data = pred_data.merge(train_data[['ogc_fid','nombre','tile_file','id','cultivo']], how='left', on=['ogc_fid','tile_file','nombre'], indicator=True)\n",
    "train_data = merged_data[merged_data._merge=='both'].drop(columns=['_merge'])\n",
    "pred_data = merged_data[merged_data._merge=='left_only'].drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfdbe9f-69c6-4ce3-99d7-ec659edecd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc5d4e-63e6-429e-9d8b-103f761bca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d309a3-f979-49bf-82a7-2e8b05d195fe",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea57b00-be8e-4751-b4b1-79890ca2c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_id2cultivo = dict((\n",
    "    train_data[['id','cultivo']]\n",
    "    .drop_duplicates()\n",
    "    .assign(id=lambda x: x.id.astype('int'))\n",
    "    .itertuples(index=False, name=None))\n",
    ")\n",
    "map_id2cultivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db2c36-a013-4133-8282-3a6392787db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_data.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee94fb21-48a6-4f1d-8157-ea254135a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_le2id = dict(zip(le.transform(le.classes_), list(map(int,le.classes_))))\n",
    "\n",
    "map_le2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08505a9b-5ec0-4763-a2a2-0e6198b8457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['id_le'] = le.transform(train_data.id)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5240870-6685-4670-a2aa-f304a7e154e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/randomforest_parameters.json','r') as f:\n",
    "    parameters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd792c8-087a-4732-a589-85dd1d20e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734d4a0-59b0-43a5-9411-1d7a7d27df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in thresholds:\n",
    "    \n",
    "    iter_train_data = deepcopy(train_data)\n",
    "    iter_pred_data = deepcopy(pred_data)\n",
    "    \n",
    "    print(f'+++++ PREDICCIONES PARA THRESHOLD {threshold}')\n",
    "    \n",
    "    threshold_folder = os.path.join('..','model',f'threshhold_{threshold}')\n",
    "    if os.path.exists(threshold_folder):\n",
    "        shutil.rmtree(threshold_folder)\n",
    "        os.mkdir(threshold_folder)\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        \n",
    "        # arma carpeta para el output (i aumenta con las iteraciones)\n",
    "        n_iter = '{0:03d}'.format(i)\n",
    "        output_folder = os.path.join(threshold_folder,f'randomforest_iterations_{n_iter}')\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # toma los datasets\n",
    "        columns = iter_train_data.filter(regex='band_').columns.to_list()\n",
    "        X_train = iter_train_data.filter(regex='band_').fillna(-99999).to_numpy()\n",
    "        y_train = iter_train_data.id_le.to_numpy()\n",
    "        X_pred = iter_pred_data.filter(regex='band_').fillna(-99999).to_numpy()\n",
    "        \n",
    "        # instancia y entrena el modelo\n",
    "        model = RandomForestClassifier(**parameters)\n",
    "        model.fit(X_train, y_train)\n",
    "        output_model_file = os.path.join(output_folder, f'model_{n_iter}.joblib')\n",
    "        _ = joblib.dump(model, output_model_file)\n",
    "        \n",
    "        # predice\n",
    "        probas = model.predict_proba(X_pred)\n",
    "        output_proba_file = os.path.join(output_folder, f'probas_{n_iter}.npy')\n",
    "        np.save(output_proba_file, probas)\n",
    "        predictions = iter_pred_data.assign(pred_class=probas.argmax(axis=1), pred_score=probas.max(axis=1))\n",
    "        \n",
    "        # separa entre nuevo train y nuevo pred\n",
    "        add_to_train = predictions.query(f'pred_score >= {threshold}').copy()\n",
    "        continue_pred = predictions.query(f'pred_score < {threshold}').copy()\n",
    "        train_data_len, add_to_train_len , continue_pred_len = iter_train_data.shape[0], add_to_train.shape[0] , continue_pred.shape[0]\n",
    "        output_pixels_file = os.path.join(output_folder, f'pixels_{n_iter}.csv')\n",
    "        (\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    [f'De entrenamiento', train_data_len],\n",
    "                    [f'Con proba>={threshold}', add_to_train_len],\n",
    "                    [f'Con proba<{threshold}', continue_pred_len]\n",
    "                ],\n",
    "                columns=['Pyxels_type','Pixels']\n",
    "            )\n",
    "            .to_csv(output_pixels_file, index=False)\n",
    "        )\n",
    "        \n",
    "        output_deptos_file = os.path.join(output_folder, f'pred_deptos_{n_iter}.csv')\n",
    "        (\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    [f'De entrenamiento', train_data_len],\n",
    "                    [f'Con proba>={threshold}', add_to_train_len],\n",
    "                    [f'Con proba<{threshold}', continue_pred_len]\n",
    "                ],\n",
    "                columns=['Pyxels_type','Pixels']\n",
    "            )\n",
    "            .to_csv(output_pixels_file, index=False)\n",
    "        )\n",
    "        \n",
    "        # pasa predicción a las columna id (target)\n",
    "        # y lo agrega al train original\n",
    "        add_to_train['id_le'] = add_to_train['pred_class']\n",
    "        add_to_train['id'] = add_to_train.id_le.apply(lambda x: map_le2id.get(x))\n",
    "        add_to_train['cultivo'] = add_to_train.id.apply(lambda x: map_id2cultivo.get(x))\n",
    "\n",
    "        iter_deptos_prediction = os.path.join(output_folder,f'randomforest__deptos_prediction_{n_iter}.csv')\n",
    "        (\n",
    "            add_to_train\n",
    "            .groupby(['cultivo','nombre'], as_index=False)\n",
    "            .size()\n",
    "            .rename(columns={'size':'pixels','nombre':'departamento'})\n",
    "            .assign(ha=lambda x: x.pixels*0.04)\n",
    "            .to_csv(iter_deptos_prediction, index=False)\n",
    "        )\n",
    "        iter_train_data =iter_train_data.append(add_to_train, ignore_index=True)\n",
    "        iter_pred_data = continue_pred\n",
    "        \n",
    "        # imprime información\n",
    "        print('''\\n*** ITERACIÓN #{0:03d}\n",
    "        - Modelo guardado en {1}\n",
    "        - Probabilidades guardadas en {2}\n",
    "        - Pixeles de entrenamiento: {3}\n",
    "        - Pixeles con proba>={4}: {5}\n",
    "        - Pixeles con proba<{4}: {6}'''.format(i, output_model_file, output_proba_file, train_data_len, threshold, add_to_train_len , continue_pred_len))\n",
    "        i += 1\n",
    "        if (add_to_train_len == 0) or (continue_pred_len == 0):\n",
    "            break\n",
    "\n",
    "    # reemplaza na en columna pred_class con 'vc_original'\n",
    "    # (los pixeles que no tiene pred_class son los pieles de verdad de campo originales)\n",
    "    # y guarda la predicción final\n",
    "    nueva_vc_prediction = os.path.join(threshold_folder,f'randomforest_nueva_vc_prediction.csv')\n",
    "    iter_train_data.to_csv(nueva_vc_prediction)\n",
    "    nueva_vc_deptos = os.path.join(threshold_folder,f'randomforest_nueva_vc_deptos.csv')\n",
    "    (\n",
    "        iter_train_data\n",
    "        .groupby(['cultivo','nombre'], as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size':'pixels','nombre':'departamento'})\n",
    "        .assign(ha=lambda x: x.pixels*0.04)\n",
    "        .to_csv(nueva_vc_deptos, index=False)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # pasa predicción a las columna id (target)\n",
    "    # y agrega las predicciones que no superaron el umbral a la nueva verdad de campo\n",
    "    # (i.e. a las que sí lo superaron)\n",
    "    # así, tenemos el conjunto completo y calculamos las ha sobre el total\n",
    "    continue_pred['id_le'] = continue_pred['pred_class']\n",
    "    continue_pred['id'] = continue_pred.id_le.apply(lambda x: map_le2id.get(x))\n",
    "    continue_pred['cultivo'] = continue_pred.id.apply(lambda x: map_id2cultivo.get(x))\n",
    "    \n",
    "    total_final_prediction = os.path.join(threshold_folder,f'randomforest_total_final_prediction.csv')\n",
    "    total_prediction = iter_train_data.append(continue_pred)\n",
    "    total_prediction.to_csv(total_final_prediction)\n",
    "    \n",
    "    total_final_deptos = os.path.join(threshold_folder,f'randomforest_total_final_deptos.csv')\n",
    "    (\n",
    "        total_prediction\n",
    "        .groupby(['cultivo','nombre'], as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size':'pixels','nombre':'departamento'})\n",
    "        .assign(ha=lambda x: x.pixels*0.04)\n",
    "        .to_csv(total_final_deptos, index=False)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
